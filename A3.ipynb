{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "Student Name: Ziyi Jiang  |  Student ID: 634926886  |  UPI: zjia631"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 -- Report\n",
    "\n",
    "###  1. Data Representation\n",
    "\n",
    "#### Method\n",
    "The data representation I used in this assignment is Bag of Words. <br/>\n",
    "Firstly, I looped through the abstracts and combined them based on class, so I got all the text for each class. Then, for each class, I split the corresponding text and count the number of each kind of word. In the end, I got a nested dictionary. The outer dictionary has the class as the key and corresponding words' occurrence number as the value. The inner dictionary has the word as the key and the corresponding occurrence number in the class as the value. <br/>\n",
    "Here is I record the BoW representation: {class{word: number of the word in the class, ...}, ...})\n",
    "\n",
    "\n",
    "#### Reason\n",
    "As a vast amount of data and I do not have much knowledge about it, it would be difficult to use other representations such as the top frequent words, N-grams. Also, the amount of each class is not equally split. Class B and E take over 90% of the dataset. Hence, using top frequent words may cause bias on the classifier. Therefore, I decided to use the bag of words data representation.\n",
    "\n",
    "\n",
    "### 2. Data Preprocessing\n",
    "\n",
    "#### Preprocessing\n",
    "1. np.genfromtxt is used to read the CSV file, and then the data is split based on different column.\n",
    "2. All the data is stored as string in NumPy ndarrys.\n",
    "3. The individual id and class are stored as one string. For example: \"1\".\n",
    "4. The ids and abstractes of tst.csv are stored. For trg.csv, only the classes and abstractes are stored.\n",
    "5. The abstracte is splited into substrings by \" \".\n",
    "6. The classifier then uses those data to find all needed parameters. (detail shown in Implementation section) <br/>\n",
    "    \\- a list of unique class &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\- total number of unique words &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\- total number of classes <br/>\n",
    "    \\- number of a word in a class &nbsp;&nbsp;&nbsp; \\- total number of each class &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\- total number of words in a class <br/>\n",
    "\n",
    "#### Reason\n",
    "The id and class are a single character, so there is no need to split them. For tst.csv we need the ids to produce the corresponding prediction. However, we do not need ids for trg.csv. We only need to check are the predictions are correct for trg.csv. Hence, the ids of trg.csv are not needed. We need to count the different words in the abstract to calculate the Naive Bayes result, so the abstract needs to be split.\n",
    "\n",
    "### 3. Extension\n",
    "The Extension used in this report is Complement Naive Bayes.\n",
    "\n",
    "#### Reason\n",
    "Here is the data of the trg.csv: <br/>\n",
    "\\- Number of each class: {'A': 128, 'B': 1602, 'E': 2144, 'V': 126} <br/>\n",
    "\\- Total word number of each class: {'A': 27529, 'B': 285505, 'E': 379088, 'V': 22700} <br/>\n",
    "We can see that both the class number and word number of B and E are significantly more than A and V. This will cause the skewed data bias, which makes the Standard Naive Bayes classifier predict more to B and E as the P(A) and P(V) would be much smaller than P(B) and P(E). <br/>\n",
    "However, the Complement Naive Bayes classifier uses the probability of the class divides the probability of a word that occurred in other classes -- P(class) / P(abstracte|other class), which could lower the influence of the skewed data bias and increase the accuracy.\n",
    "\n",
    "For example, if P(abstracte|class1) = 0.8 and P(abstracte|class2) = 0.2 but P(class1) = 0.1 and P(class2)= 0.6. And P(abstracte|classes other than class1) = 0.1 and P(abstracte|classes other than class2) = 0.9 Then, because P(class1) * P(abstracte|class1) = 0.08 < P(class2) * P(abstracte|class2) = 0.12, the Standard Naive Bayes classifier would predict this abstracte as class2. However, the Complement Naive Bayes classifier would calculates P(class1) / P(abstracte|classes other than class1) = 1 > P(class2) / P(abstracte|classes other than class2) = 0.667, and hence assign this abstracte to class1.\n",
    "\n",
    "Therefore, since there is significantly skewed data bias in this dataset, Complement Naive Bayes is a suitable extension.\n",
    "\n",
    "### 4. Implementation\n",
    "Due to the 0 probability problem, I use 1 and total unique words number as smoothing parameters:<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "P(class|abstracte) = P(class) $\\times$ P(abstracte|class) = P(class) $\\times$ $\\prod_{}^{}$P(word|class) = P(class) $\\times$ $\\prod_{}^{}$$\\frac{\\text{word number } + 1}{\\text{total words number in class } + \\text{ total unique words number}}$\n",
    "\n",
    "Due to the underflow problem, the code uses np.log() to record the probabilities (log-recorded probability):<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "log-recorded P(class|abstracte) = log-recorded P(class) + log-recorded P(abstracte|class) <br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "= log-recorded P(class) + $\\sum_{}^{}$log-recorded P(word|class) <br/> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "= $\\log$$\\frac{\\text{number of the class}}{\\text{total number of classes}}$ + $\\sum_{}^{}$$\\log$$\\frac{\\text{word number } + 1}{\\text{total words number in class } + \\text{ total unique words number}}$\n",
    "\n",
    "For both classifier, I created two classes called StandardNaiveBayesClassifier and ComplementNaiveBayesClassifier. There are two methods in each class -- fit() and predict(). The fit() method record 6 parameters that are needed by the Naive Bayes. The predict method uses those parameters to implement different vision of Naive Bayes.\n",
    "\n",
    "#### Standard Naive Bayes Classifier\n",
    "The Standard Naive Bayes classifier uses the formula: P(class|abstracte) = P(class) * P(abstracte|class) <br/>\n",
    "***Detail:*** <br/>\n",
    "Firstly, I recorded 6 parameter:\n",
    "1. a list of unique class ([\"A\", \"B\", \"E\", \"V\"]) &nbsp;&nbsp;&nbsp; 4. total number of unique words (a dictionary -- {word: number of the word, ...})\n",
    "2. total number of classes (an integer) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5. number of a word in a class (a dictionary of dictionary -- {class{word: number of the word in the class, ...}, ...})\n",
    "3. total number of each class (an integer) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6. total number of words in a class (an integer)\n",
    "\n",
    "Then. I use those parameters to calculate the log-recorded P(class|abstracte) of each class:\n",
    "1. calculate log-recorded P(class) by using np.log(number of the class / total number of classes)\n",
    "2. calculate log-recorded P(abstracte|class) by sum all log(P(word|class)) of each word in the abstract\n",
    "3. use log(P(class)) + log(P(abstracte|class)) to get the log-recorded P(class|abstracte)\n",
    "4. find the class with the maximum log-recorded P(class|abstracte) -- this would be the predicted class\n",
    "\n",
    "#### Complement Naive Bayes Classifier\n",
    "The Complement Naive Bayes classifier uses the formula: P(class|abstracte) = P(class) / P(abstracte|other class) <br/>\n",
    "***Detail:*** <br/>\n",
    "Firstly, I recorded the same 6 parameters as in the Standard Naive Bayes classifier.<br/>\n",
    "Then, I calculate the P(class|abstracte) of each class:\n",
    "1. the calculation of log-recorded P(class) is the same as in the Standard Naive Bayes classifier\n",
    "2. calculate log-recorded P(abstracte|other class) by sum all log(P(word|other class)) of each word in the abstract:\n",
    "    - calculate P(word|other class) by using <br/> (sum the number of the word in all other classes + 1) / (sum the total number of words in all other classes + total number of unique words)\n",
    "3. use log(P(class)) - log(P(abstracte|other class)) to get the log-recorded P(class|abstracte)\n",
    "4. find the class with the maximum log-recorded P(class|abstracte) -- this would be the predicted class\n",
    "\n",
    "### 5. Result\n",
    "\n",
    "#### Test Method (Training and Validation)\n",
    "Split Method: <br/>\n",
    "I wrote a function named stratified_cv_split() which split the data into k folds based on the percentage of each class. The function first split the indices based on different classes. Then, it randomly split those indices into k subgroups for each class. After that, the function uses those indices to extract the corresponding abstract and class. <br/>\n",
    "Because the function uses stratified cross-validation, the split data will have similar distribution with the whole data.\n",
    "\n",
    "Get Accuracy Method: <br/>\n",
    "The get_accuracy() function converts the result into true and false based on the condition (actual class = predicted class). Then, the function counts how many of the results are True using a sum() function. And use the count / number of results to get and return the accuracy.\n",
    "\n",
    "#### Comparision\n",
    "\n",
    "I used a loop to do the stratified cross-validation 20 times. In each loop, I use stratified_cv_split() to do 5-folds stratified cross-validation, use the same split data for the two classifiers, and record the accuracies. After the loop, I calculate the mean and make the comparison. Here is the test result on the training set: <br/>\n",
    "Standard Naive Bayes Classifier mean accuracy: 0.9439314871336758 <br/>\n",
    "Complement Naive Bayes mean accuracy: 0.9617133179483361 <br/>\n",
    "Difference of mean accuracies (Complement - Standard): 0.017781830814660338 <br/>\n",
    "\n",
    "For the accuracy of tst.csv, Kaggle shows the Standard Naive Bayes classifier's accuracy is 0.95333 and Complement Naive Bayes classifier's accuracy is 97.\n",
    "\n",
    "Therefore, for this dataset, the Complement Naive Bayes classifier is more accurate than the Standard Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 -- Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Unique class is stored as self.unique_class.\n",
    "        Total number of classes is stored as self.total_classes_number.\n",
    "        Total number of each class is stored in self.class_number_dict.\n",
    "        Total number of unique words is stored as self.unique_word_number.\n",
    "        Total number of a word in a class is stored in self.class_word_number_dict.\n",
    "        otal number of a word in a class is stored in self.class_total_words_dict.\n",
    "        \"\"\"\n",
    "        self.unique_classes = []\n",
    "        self.total_classes_number = 0\n",
    "        self.class_number_dict = {}\n",
    "        self.unique_word_number = 0\n",
    "        self.class_word_number_dict = {}\n",
    "        self.class_total_words_dict = {}\n",
    "\n",
    "    def fit(self, abstracts, classes):\n",
    "        classes = np.array(classes)\n",
    "        abstracts = np.array(abstracts)\n",
    "        # unique class\n",
    "        self.unique_classes = sorted(set(classes))\n",
    "\n",
    "        # total number of classes\n",
    "        self.total_classes_number = len(classes)\n",
    "\n",
    "        # total number of each class\n",
    "        c, counts = np.unique(classes, return_counts=True)\n",
    "        self.class_number_dict = dict(zip(c, counts))\n",
    "\n",
    "        # total number of unique words\n",
    "        all_text = \"\"\n",
    "        for text in abstracts:\n",
    "            all_text = all_text + text + \" \"\n",
    "        all_text = all_text.split()\n",
    "        unique_word = set(all_text)\n",
    "        self.unique_word_number = len(unique_word)\n",
    "\n",
    "        # total number of a word in a class\n",
    "        self.class_word_number_dict = {}\n",
    "        class_text_dict = {}\n",
    "        for c in self.unique_classes:\n",
    "            class_text_dict[c] = \"\"\n",
    "        for i in range(len(abstracts)):\n",
    "            c = classes[i]\n",
    "            text = abstracts[i]\n",
    "            class_text_dict[c] = class_text_dict[c] + text + \" \"\n",
    "        for c in class_text_dict:\n",
    "            text = np.array(class_text_dict[c].split())\n",
    "            word, counts = np.unique(text, return_counts=True)\n",
    "            self.class_word_number_dict[c] = dict(zip(word, counts))\n",
    "\n",
    "        # total number of words in a class\n",
    "        self.class_total_words_dict = {}\n",
    "        for c in class_text_dict:\n",
    "            text = class_text_dict[c].split()\n",
    "            self.class_total_words_dict[c] = len(text)\n",
    "\n",
    "    def predict(self, abstracts, ids):\n",
    "        result = []\n",
    "        for i in range(len(ids)):\n",
    "            pred_id = ids[i]\n",
    "            pred_abs = abstracts[i].split()\n",
    "            id_result_prob = []\n",
    "            for c in self.unique_classes:\n",
    "                prob_c = np.log(self.class_number_dict[c] / self.total_classes_number)\n",
    "                prob_x = 0\n",
    "                word_number_dict = self.class_word_number_dict[c]\n",
    "                total_words_number = self.class_total_words_dict[c]\n",
    "                for word in pred_abs:\n",
    "                    if word in word_number_dict:\n",
    "                        word_number = word_number_dict[word]\n",
    "                    else:\n",
    "                        word_number = 0\n",
    "                    prob_word = np.log((word_number + 1) / (total_words_number + self.unique_word_number))\n",
    "                    prob_x = prob_x + prob_word\n",
    "                prob = prob_c + prob_x\n",
    "                id_result_prob.append([c, prob])\n",
    "\n",
    "            id_result_prob = sorted(id_result_prob, key=lambda x: x[1])\n",
    "            result.append((pred_id, id_result_prob[-1][0]))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navie Bayes Classifier with Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplementNaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.unique_classes = []\n",
    "        self.total_classes_number = 0\n",
    "        self.class_number_dict = {}\n",
    "        self.unique_word_number = 0\n",
    "        self.class_word_number_dict = {}\n",
    "        self.class_total_words_dict = {}\n",
    "\n",
    "    def fit(self, abstracts, classes):\n",
    "        classes = np.array(classes)\n",
    "        abstracts = np.array(abstracts)\n",
    "        \n",
    "        self.unique_classes = sorted(set(classes))\n",
    "\n",
    "        self.total_classes_number = len(classes)\n",
    "\n",
    "        c, counts = np.unique(classes, return_counts=True)\n",
    "        self.class_number_dict = dict(zip(c, counts))\n",
    "\n",
    "        all_text = \"\"\n",
    "        for text in abstracts:\n",
    "            all_text = all_text + text + \" \"\n",
    "        all_text = all_text.split()\n",
    "        unique_word = set(all_text)\n",
    "        self.unique_word_number = len(unique_word)\n",
    "\n",
    "        self.class_word_number_dict = {}\n",
    "        class_text_dict = {}\n",
    "        for c in self.unique_classes:\n",
    "            class_text_dict[c] = \"\"\n",
    "        for i in range(len(abstracts)):\n",
    "            c = classes[i]\n",
    "            text = abstracts[i]\n",
    "            class_text_dict[c] = class_text_dict[c] + text + \" \"\n",
    "        for c in class_text_dict:\n",
    "            text = np.array(class_text_dict[c].split())\n",
    "            word, counts = np.unique(text, return_counts=True)\n",
    "            self.class_word_number_dict[c] = dict(zip(word, counts))\n",
    "\n",
    "        self.class_total_words_dict = {}\n",
    "        for c in class_text_dict:\n",
    "            text = class_text_dict[c].split()\n",
    "            self.class_total_words_dict[c] = len(text)\n",
    "\n",
    "    def predict(self, abstracts, ids):\n",
    "        result = []\n",
    "        for i in range(len(ids)):\n",
    "            pred_id = ids[i]\n",
    "            pred_abs = abstracts[i].split()\n",
    "            id_result_prob = []\n",
    "            for c in self.unique_classes:\n",
    "                prob_c = np.log(self.class_number_dict[c] / self.total_classes_number)\n",
    "                prob_x = 0\n",
    "                \"\"\"\n",
    "                Extension\n",
    "                \"\"\"\n",
    "                not_cs = []\n",
    "                word_number_in_ncs = []\n",
    "                total_words_number = 0\n",
    "                for nc in self.unique_classes:\n",
    "                    if nc != c:\n",
    "                        not_cs.append(nc)\n",
    "                        word_number_in_ncs.append(self.class_word_number_dict[nc])\n",
    "                        total_words_number += self.class_total_words_dict[nc]  \n",
    "                for word in pred_abs:\n",
    "                    word_number = 0\n",
    "                    for word_number_dict in word_number_in_ncs:\n",
    "                        if word in word_number_dict:\n",
    "                            word_number += word_number_dict[word]\n",
    "                    prob_word = np.log((word_number + 1) / (total_words_number + self.unique_word_number))\n",
    "                    prob_x = prob_x + prob_word\n",
    "                prob = prob_c - prob_x\n",
    "                id_result_prob.append([c, prob])\n",
    "            id_result_prob = sorted(id_result_prob, key=lambda x: x[1])\n",
    "            result.append((pred_id, id_result_prob[-1][0]))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_cv_split(words, classes, randomseed, k=5):\n",
    "    words = np.array(words)\n",
    "    classes = np.array(classes)\n",
    "    class_index_dict = {}\n",
    "    for i in range(len(classes)):\n",
    "        if classes[i] in class_index_dict:\n",
    "            class_index_dict[classes[i]].append(i)\n",
    "        else:\n",
    "            class_index_dict[classes[i]] = [i]\n",
    "    class_split_result = []\n",
    "    word_split_result = []\n",
    "    split_percent = 10 / k / 10\n",
    "    for c in class_index_dict:\n",
    "        randomseed += 1\n",
    "        c_indices = np.array(class_index_dict[c])\n",
    "        rng = np.random.default_rng(seed = randomseed)\n",
    "        indices = rng.permutation(len(c_indices))\n",
    "        c_split_result = []\n",
    "        w_split_result = []\n",
    "        split_point1 = 0\n",
    "        split_point2 = int(len(c_indices) * split_percent)\n",
    "        for i in range(k):\n",
    "            if i == k-1:\n",
    "                indexs = indices[split_point1:]\n",
    "                indexs = c_indices[indexs]\n",
    "                c_split_result.append(classes[indexs])\n",
    "                w_split_result.append(words[indexs])\n",
    "            else:\n",
    "                indexs = indices[split_point1:split_point2]\n",
    "                indexs = c_indices[indexs]\n",
    "                c_split_result.append(classes[indexs])\n",
    "                w_split_result.append(words[indexs])\n",
    "            split_part = int(len(c_indices) * split_percent)\n",
    "            split_point1 += split_part\n",
    "            split_point2 += split_part\n",
    "        class_split_result.append(c_split_result)\n",
    "        word_split_result.append(w_split_result)\n",
    "    result = []\n",
    "    for i in range(k):\n",
    "        x_test = np.array([])\n",
    "        y_test = np.array([])\n",
    "        x_train = np.array([])\n",
    "        y_train = np.array([])\n",
    "        for c in range(len(class_split_result)):\n",
    "            x_test = np.concatenate((x_test, word_split_result[c][i]))\n",
    "            y_test = np.concatenate((y_test, class_split_result[c][i]))\n",
    "        for j in range(k):\n",
    "            if j != i:\n",
    "                for c in range(len(class_split_result)):\n",
    "                    x_train = np.concatenate((x_train, word_split_result[c][j]))\n",
    "                    y_train = np.concatenate((y_train, class_split_result[c][j]))\n",
    "        result.append((x_train, x_test, y_train, y_test))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(result):\n",
    "    result = np.array(result)\n",
    "    count = sum(result[:, 0] == result[:, 1])\n",
    "    accuracy = count / len(result)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "train_abstracts = []\n",
    "training_data = np.genfromtxt(\"trg.csv\", delimiter=\",\",  skip_header=1, dtype=str, usecols=(1, 2))\n",
    "classes = training_data[:, 0]\n",
    "train_abstracts = training_data[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Naive Bayes Classifier mean: 0.9439314871336758\n",
      "Complement Naive Bayes mean: 0.9617133179483361\n",
      "Difference of Means (Complement - Standard): 0.017781830814660338\n"
     ]
    }
   ],
   "source": [
    "nbc_scores = []\n",
    "cnbc_scores = []\n",
    "for i in range(20):\n",
    "    scv_split = stratified_cv_split(train_abstracts, classes, i)\n",
    "    for data in scv_split:\n",
    "        x_train = data[0]\n",
    "        x_test = data[1]\n",
    "        y_train = data[2]\n",
    "        y_test = data[3]\n",
    "        nbc =NaiveBayesClassifier()\n",
    "        nbc.fit(x_train, y_train)\n",
    "        nbc_result = nbc.predict(x_test, y_test)\n",
    "        nbc_accuracy = get_accuracy(nbc_result)\n",
    "        nbc_scores.append(nbc_accuracy)\n",
    "    \n",
    "        cnbc =ComplementNaiveBayesClassifier()\n",
    "        cnbc.fit(x_train, y_train)\n",
    "        cnbc_result = cnbc.predict(x_test, y_test)\n",
    "        cnbc_accuracy = get_accuracy(cnbc_result)\n",
    "        cnbc_scores.append(cnbc_accuracy)\n",
    "\n",
    "print(\"Standard Naive Bayes Classifier mean:\", np.mean(nbc_scores))\n",
    "print(\"Complement Naive Bayes mean:\", np.mean(cnbc_scores))\n",
    "print(\"Difference of Means (Complement - Standard):\", np.mean(cnbc_scores) - np.mean(nbc_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result of tst.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = []\n",
    "train_abstracts = []\n",
    "training_data = np.genfromtxt(\"trg.csv\", delimiter=\",\",  skip_header=1, dtype=str, usecols=(1, 2))\n",
    "train_classes = training_data[:, 0]\n",
    "train_abstracts = training_data[:, 1]\n",
    "\n",
    "ids = []\n",
    "abstracts = []\n",
    "data = np.genfromtxt(\"tst.csv\", delimiter=\",\",  skip_header=1, dtype=str, usecols=(0, 1))\n",
    "ids = data[:, 0]\n",
    "abstracts = data[:, 1]\n",
    "\n",
    "nbc = NaiveBayesClassifier()\n",
    "nbc.fit(train_abstracts, train_classes)\n",
    "result = nbc.predict(abstracts, ids)\n",
    "\n",
    "f = open(\"nbc_result.csv\", \"a\")\n",
    "f.write(\"id,class\\n\")\n",
    "for r in result:\n",
    "    line = r[0] + \",\" + r[1] + \"\\n\"\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = []\n",
    "train_abstracts = []\n",
    "training_data = np.genfromtxt(\"trg.csv\", delimiter=\",\",  skip_header=1, dtype=str, usecols=(1, 2))\n",
    "train_classes = training_data[:, 0]\n",
    "train_abstracts = training_data[:, 1]\n",
    "\n",
    "ids = []\n",
    "abstracts = []\n",
    "data = np.genfromtxt(\"tst.csv\", delimiter=\",\",  skip_header=1, dtype=str, usecols=(0, 1))\n",
    "ids = data[:, 0]\n",
    "abstracts = data[:, 1]\n",
    "\n",
    "cnbc = ComplementNaiveBayesClassifier()\n",
    "cnbc.fit(train_abstracts, train_classes)\n",
    "result = cnbc.predict(abstracts, ids)\n",
    "f = open(\"cnbc_result.csv\", \"a\")\n",
    "f.write(\"id,class\\n\")\n",
    "for r in result:\n",
    "    line = r[0] + \",\" + r[1] + \"\\n\"\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
